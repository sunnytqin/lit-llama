
import logging
import sys, os
from pathlib import Path
import pickle
import random
import sys
import time
from typing import Iterator, Tuple
import warnings
import numpy as np
import json

import lightning as L
import torch
import torch.nn as nn

from lit_llama import LLaMA, Tokenizer
from lit_llama.model import pipeLLaMA, LLaMAConfig
from lit_llama.utils import EmptyInitOnDevice, jsd


DTYPE = torch.float32
DEVICE = torch.device("cuda:0")

class PrecomputedShardLoader:
    def __init__(self, 
        shard_dirs: list[str],
    ):
        """
            Loads shards generated by precompute_logits.py and yields examples one by one.

            Args:
                shard_dirs: 
                    List of directories containing (only) sets of 
                    corresponding shards computed by precompute_logits.py
                shuffle_shards: 
                    Whether to shuffle the shards
                shuffle_seed: 
                    Seed for shuffling the shards
        """
        self.shard_dirs = shard_dirs

        shard_name_lists = []
        for shard_dir in self.shard_dirs:
            shards = os.listdir(shard_dir)

            # Shard names are assumed to be in the format "name_number.pickle"
            shards = list(sorted(shards, key=lambda x: int(x.split('_')[-1].strip('.pickle'))))

            shard_name_lists.append(shards)

        l = len(shard_name_lists[0])
        assert(all([len(shard_name_list) == l for shard_name_list in shard_name_lists]))

        shards = list(zip(*shard_name_lists))
        self.shards = shards

        prompts_json_path = "/n/holyscratch01/barak_lab/Lab/gahdritz/wikipedia/wiki_test.json"
        with open(prompts_json_path, "r") as fp:
            prompts = json.load(fp)

        self.prompts = list(sorted(prompts.items(), key=lambda t: t[0]))

    def load_shards(self, shard_id: int):
        shards = []
        for shard_dir, shard_name in zip(self.shard_dirs, self.shards[shard_id]):
            shard_path = os.path.join(shard_dir, shard_name)
            with open(shard_path, "rb") as fp:
                shard = pickle.load(fp)

            shards.append(shard)

        return shards

    def shuffle_shards(self, seed: int):
        random.Random(seed).shuffle(self.shards)

    def __iter__(self):
        return self._gen()

    def _gen(self):
        """
            Returns a generator that yields tuples of examples one by one.
        """
        cur_shard_id = 0
        while cur_shard_id < len(self.shards):
            t = time.time()
            # Load corresponding shards
            t = time.time()
            logging.info(f"Loading shards...")
            loaded_shards = self.load_shards(cur_shard_id)
            logging.info(f"Shards loaded ({time.time() - t:.02f} seconds)...")

            # All shards in the tuple should be of the same length
            shard_len = len(loaded_shards[0])
            assert(all([len(shard) == shard_len for shard in loaded_shards]))

            # Sort examples within each shard by key
            sort_shard = lambda l: list(sorted(l.items(), key=lambda t: t[0]))

            for i in range(len(loaded_shards)):
                loaded_shards[i] = sort_shard(loaded_shards[i])
                for j in range(len(loaded_shards[i])):
                    loaded_shards[i][j] = (*loaded_shards[i][j], self.prompts[j][1])

            yield from zip(*loaded_shards)

            cur_shard_id += 1

            del loaded_shards


def load_lm_head(checkpoint_path: str, dtype):
    # Load the small model's LM head
    logging.info(f"Loading model at {checkpoint_path}... ")
    t = time.time()
    checkpoint = torch.load(checkpoint_path)
    assert(len([k for k in checkpoint.keys() if "lm_head" in k]) == 1)
    lm_head_weights = checkpoint["lm_head.weight"]
    vocab_size, emb_dim = lm_head_weights.shape
    lm_head = nn.Linear(
        emb_dim, vocab_size, bias=False
    )
    with torch.no_grad():
        lm_head.weight.data = lm_head_weights.to(dtype)
        lm_head.eval()
        lm_head = lm_head.to(DEVICE)

    logging.info(f"Time: {time.time() - t:.02f} seconds.")

    del checkpoint

    return lm_head


def _preprocessor(
    shard_loader: PrecomputedShardLoader,
    small_lm_head: nn.Linear,
    large_lm_head: nn.Linear,
    device: torch.device,
    tokenizer: Tokenizer,
):
    for i, (small_tup, large_tup) in enumerate(shard_loader):
        small_key, small_emb, prompt = small_tup
        large_key, large_emb, prompt  = large_tup

        # Sanity check. The shards should be aligned such that the keys match.
        assert(small_key == large_key)

        # Some empty articles slipped through my filter. Sad!
        if (small_emb.shape[0] == 1):
            continue

        # assert((torch.sum(large_emb, dim=1) > 0.0).all())
        
        small_emb = small_emb.to(device=device, dtype=DTYPE)
        large_emb = large_emb.to(device=device, dtype=DTYPE)
        
        with torch.no_grad():
            # Compute logits from the small model embeddings
            small_logits = small_lm_head(small_emb)
            large_logits = large_lm_head(large_emb)

            # Softmax both sets of logits
            small_logits = torch.nn.functional.softmax(small_logits, dim=-1)
            large_logits = torch.nn.functional.softmax(large_logits, dim=-1)

            # compute logits 
            small_entropy = -torch.sum(torch.log(small_logits) * small_logits, dim=1)
            large_entropy = -torch.sum(torch.log(large_logits) * large_logits, dim=1)

            # Compute the JS divergence between the two distributions
            divergence = jsd(small_logits, large_logits)

            top_k_small = torch.topk(small_logits, 100, dim=-1)
            top_k_large = torch.topk(large_logits, 100, dim=-1)

            # assert((torch.sum(large_logits, dim=1) > 0.999).all())

        encoded_prompt = tokenizer.sp_encode(prompt).pieces
        encoded_prompt = [p.piece for p in encoded_prompt]

        yield (top_k_small[1], top_k_large[1], top_k_small[0], top_k_large[0], small_entropy, large_entropy, divergence, encoded_prompt)


def batch_loader(
    data_gen: Iterator[Tuple[torch.Tensor, torch.Tensor]],
    batch_size: int,
):
    batch = []
    for i, (small_idx, large_idx, small_probs, large_probs, small_h, large_h, jsd, prompt) in enumerate(data_gen):
        # [N, emb_dim]
        assert(len(small_idx.shape) == 2)
        small_idx = torch.unbind(small_idx, dim=-2)
        large_idx = torch.unbind(large_idx, dim=-2)
        small_probs = torch.unbind(small_probs, dim=-2)
        large_probs = torch.unbind(large_probs, dim=-2)

        # [N]
        assert(len(jsd.shape) == 1)
        small_h = torch.unbind(small_h, dim=-1)
        large_h = torch.unbind(large_h, dim=-1)
        jsd = torch.unbind(jsd, dim=-1)

        # [str]
        # prompt = list(prompt)

        assert(len(small_idx) == len(jsd))

        for si, li, sp, lp, sh, lh, jsd, pt in zip(small_idx, large_idx, small_probs, large_probs, small_h, large_h, jsd, prompt):
            batch.append((si, li, sp, lp, sh, lh, jsd, pt))

            if(len(batch) == batch_size):
                si_b = torch.stack([t[0] for t in batch])
                li_b = torch.stack([t[1] for t in batch])
                sp_b = torch.stack([t[2] for t in batch])
                lp_b = torch.stack([t[3] for t in batch])
                sh_b = torch.stack([t[4] for t in batch])
                lh_b = torch.stack([t[5] for t in batch])
                jsd_b = torch.stack([t[6] for t in batch])
                pt_b = np.stack([t[7] for t in batch])

                yield si_b, li_b, sp_b, lp_b, sh_b, lh_b, jsd_b, pt_b

                batch = []

    # Toss the last batch if it's too small
    pass


def main(
    *,
    precomputed_small_emb_dir: str='/n/holyscratch01/barak_lab/Lab/gahdritz/wikipedia/wiki_logits/7B_test',
    precomputed_large_emb_dir: str='/n/holyscratch01/barak_lab/Lab/gahdritz/wikipedia/wiki_logits/30B_test',
    output_file: str="sample_output/teacher_sample.npz",
    small_checkpoint_path: str = None,
    large_checkpoint_path: str = None,
    tokenizer_path: str=None,
    batch_size: int = 512,
    seed: int = 42,
) -> None:
    """
    Args:
        precomputed_small_emb_dir: Directory containing embeddings for the small model (generated by precompute_logits.py)
        precomputed_large_emb_dir: Directory containing embeddings for the large model (generated by precompute_logits.py)
        output_dir: Where to save output files
        checkpoint_path: The small LM checkpoint path.
        batch_size: Batch size.
    """
    torch.manual_seed(seed)
    random.seed(seed)

    if not small_checkpoint_path:
        small_checkpoint_path = Path(f'/n/holystore01/LABS/barak_lab/Everyone/checkpoints/checkpoints/lit-llama/7B/state_dict.pth')
    if not large_checkpoint_path:
        large_checkpoint_path = Path(f'/n/holystore01/LABS/barak_lab/Everyone/checkpoints/checkpoints/lit-llama/30B/state_dict.pth')
    if not tokenizer_path:
        tokenizer_path = Path('/n/holystore01/LABS/barak_lab/Everyone/checkpoints/checkpoints/lit-llama/tokenizer.model')    

    assert small_checkpoint_path.is_file()
    assert large_checkpoint_path.is_file()

    small_lm_head = load_lm_head(small_checkpoint_path, DTYPE)
    print("small lm head loaded")
    sys.stdout.flush()
    large_lm_head = load_lm_head(large_checkpoint_path, DTYPE)
    print("large lm head loaded")
    sys.stdout.flush()

    # Load the precomputed logits
    logit_loader = PrecomputedShardLoader(
        [
            precomputed_small_emb_dir,
            precomputed_large_emb_dir
        ],
    )

    tokenizer = Tokenizer(tokenizer_path)

    # Lopping through data
    data_gen = _preprocessor(
        shard_loader=logit_loader,
        small_lm_head=small_lm_head,
        large_lm_head=large_lm_head,
        device=DEVICE,
        tokenizer=tokenizer, 
    )

    bl = batch_loader(
        data_gen=data_gen,
        batch_size=1,
    )

    small_sentence_pieces = []
    large_sentence_pieces = []
    small_sentence_probs = []
    large_sentence_probs = []
    small_entropy = []
    large_entropy = []
    jsd_distances = []
    prompts = []


    for i, (small_idx, large_idx, small_probs, large_probs, small_h, large_h, jsd, prompt) in enumerate(bl):
        small_sentence = tokenizer.decode(small_idx[0])
        large_sentence = tokenizer.decode(large_idx[0])
        
        small_sentence_pieces.append([p.piece for p in small_sentence.pieces])
        large_sentence_pieces.append([p.piece for p in large_sentence.pieces])
        

        small_sentence_probs.append(small_probs.cpu().numpy().squeeze())
        large_sentence_probs.append(large_probs.cpu().numpy().squeeze())

        small_entropy.append(small_h.cpu().numpy())
        large_entropy.append(large_h.cpu().numpy())
        
        jsd_distances.append(jsd.cpu().numpy())

        prompts.append(prompt)
        
        if i>5_000:
            break
    
    # Save the model
    np.savez(output_file,
             small_sentence_probs=np.array(small_sentence_probs),
             large_sentence_probs=np.array(large_sentence_probs),
             small_sentence_pieces=np.array(small_sentence_pieces),
             large_sentence_pieces=np.array(large_sentence_pieces),
             small_sentence_entropy=np.array(small_entropy),
             large_sentence_entropy=np.array(large_entropy), 
             JSD=jsd_distances,
             prompts=np.array(prompts)
             )

if __name__ == "__main__":
    from jsonargparse import CLI

    torch.set_float32_matmul_precision("high")
    warnings.filterwarnings(
        # Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31
        "ignore", 
        message="ComplexHalf support is experimental and many operators don't support it yet"
    )
    warnings.filterwarnings(
        # Triggered in bitsandbytes/autograd/_functions.py:298
        "ignore", 
        message="MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization",
    )
    warnings.filterwarnings(
        # SLURM srun warning
        "ignore", 
        message="The `srun` command is available on your system but is not used",
    )

    CLI(main)