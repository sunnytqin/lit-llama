import os
from pathlib import Path
import random

import lightning as L
import torch
import torch.nn as nn

import matplotlib.pyplot as plt
from generate_from_logits import load_lm_head
from repetition import compute_entropy

DEVICE="cuda"
# DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32
DTYPE = torch.float32

def main(
    *,
    repetition_dir: str, 
    output_dir: str,
    small_checkpoint_path: str = None,
    seed: int = 42,
    model_size: str = "7B"
) -> None:
    """
    Args:
        repetition_dir: director to find repetition.py (generated by precompute_logits.py)
        output_dir: Where to save output files
        checkpoint_path: The small LM checkpoint path.
        batch_size: Batch size.
    """
    # MUST COME FIRST
    args = locals()

    torch.manual_seed(seed)
    random.seed(seed)

    if not small_checkpoint_path:
        small_checkpoint_path = Path(f"/n/holystore01/LABS/barak_lab/Everyone/checkpoints/checkpoints/lit-llama/{model_size}/lit-llama.pth")
    assert small_checkpoint_path.is_file()

    lm_head = load_lm_head(small_checkpoint_path, DTYPE)
    print("small lm head loaded")

    repetition_file_path = os.path.join(repetition_dir, "repetition.pt")

    data = torch.load(repetition_file_path, map_location=DEVICE)
    new_embed_all = data["new_embed"].to(DTYPE)
    original_embed_all = torch.stack(data["original_embed"]).to(DTYPE)
    n_samples = new_embed_all.shape[0]
    large_entropy = torch.concatenate(data["large_entropy"][0: n_samples]).cpu()
    print("embedding shape: ", original_embed_all.shape, new_embed_all.shape)
    prompt_type = (data["prompt_type"][0: n_samples]).bool().cpu()

    original_logits = lm_head(original_embed_all).detach()
    new_logits = lm_head(new_embed_all).detach()
    print("logits shape: ", original_logits.shape, new_logits.shape)

    original_probs = torch.softmax(original_logits, dim=-1)
    new_probs = torch.softmax(new_logits, dim=-1)

    original_entropy = compute_entropy(original_logits.cpu())
    new_entropy = compute_entropy(new_logits.cpu())

    global img_output_dir
    img_output_dir = output_dir

    plot_entropy(original_entropy.numpy(), new_entropy.numpy(), large_entropy.numpy(), prompt_type.numpy())

    mutual_information(original_probs, new_probs, prompt_type)

    probs_norm(original_probs, new_probs, prompt_type)

    kl_divergence(original_probs, new_probs, prompt_type)

    weighted_entropy(original_entropy, new_entropy, original_probs, prompt_type)

    return

def weighted_entropy(original_entropy, new_entropy, original_probs, prompt_type):
    px = torch.topk(original_probs, 10, dim=1)
    p_x = px.values
    p_x = torch.div(p_x, p_x.sum(dim=1, keepdim=True)).cpu()

    weighted_entropy = (new_entropy * p_x).sum(dim=1)

    plt.figure()
    plt.hist(weighted_entropy[prompt_type].cpu().numpy()/ original_entropy[prompt_type].cpu().numpy(), bins=30, histtype="step", label="low_e_high_a")
    plt.hist(weighted_entropy[~prompt_type].cpu().numpy()/original_entropy[~prompt_type].cpu().numpy(), bins=30, histtype="step", label="high_e_low_a")
    plt.xlabel("Weighted Entropy")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig(f"{img_output_dir}/weighted_entropy.png", bbox_inches='tight')

    print("--- weighted entropy classifier ---")
    simple_classification(weighted_entropy.to(DEVICE), prompt_type.to(DEVICE))

    return 


def probs_norm(x_probs, y_probs, prompt_type):
    px = torch.topk(x_probs, 10, dim=1)
    top_k_idx = px.indices
    p_x = px.values
    p_x = torch.div(p_x, p_x.sum(dim=1, keepdim=True))

    p_y_given_x = torch.empty_like(p_x)
    for i in range(top_k_idx.shape[0]):
        for j in range(top_k_idx.shape[1]):
            ii_idx = top_k_idx[i, j]
            p_y_given_x[i, j] = y_probs[i, j, ii_idx]

    norm = torch.sum(p_x * p_y_given_x, dim=1)
    print("norm shape: ", norm.shape)

    plt.figure()
    plt.hist(norm[prompt_type].cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist(norm[~prompt_type].cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("Probability Norm")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig(f"{img_output_dir}/probs_norm.png", bbox_inches='tight')

    return


def kl_divergence(x_probs, y_probs, prompt_type):
    px = torch.topk(x_probs, 10, dim=1)
    p_x = px.values
    p_x = torch.div(p_x, p_x.sum(dim=1, keepdim=True))

    p_y_given_x = y_probs

    p_xy = torch.mul(p_x.unsqueeze(-1), p_y_given_x)
    
    p_y = torch.sum(p_xy, dim=1)

    total_m = 0.5 * (x_probs + p_y)
    kl = nn.functional.kl_div(total_m, x_probs, reduction="none", log_target=True)
    kl += nn.functional.kl_div(total_m, p_y, reduction="none", log_target=True)
    kl = 0.5 * kl.sum(dim=-1)

    plt.figure()
    plt.hist(kl[prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist(kl[~prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("KL Divergence")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig(f"{img_output_dir}/kl_div.png", bbox_inches='tight')
    
    print("--- JSD classifier ---")
    simple_classification(kl.to(DEVICE), prompt_type.to(DEVICE))

    return


def mutual_information(x_probs, y_probs, prompt_type):
    px = torch.topk(x_probs, 10, dim=1)
    top_k_idx = px.indices
    p_x = px.values
    p_x = torch.div(p_x, p_x.sum(dim=1, keepdim=True))

    print(top_k_idx.shape, y_probs.shape)

    # # if choose to restrict the support of Y to only top K tokens
    # p_y_given_x = torch.empty(p_x.shape[0], p_x.shape[1], p_x.shape[1], device=p_x.device)
    # for i in range(top_k_idx.shape[0]):
    #     ii_idx = top_k_idx[i]
    #     for j in range(top_k_idx.shape[1]):
    #         p_y_given_x[i, j, :] = y_probs[i, j, ii_idx]
    # p_y_given_x = torch.div(p_y_given_x, p_y_given_x.sum(dim=2, keepdim=True))
    
    # if choose support of Y to be all tokens
    p_y_given_x = y_probs

    p_xy = torch.mul(p_x.unsqueeze(-1), p_y_given_x)
    
    HYX = -torch.sum(p_xy * p_y_given_x.log(), dim=[1, 2])

    p_y = torch.sum(p_xy, dim=1)
    HY = -torch.sum(p_y * p_y.log(), dim=1)

    MI = HY - HYX

    plt.figure(figsize=(12, 12))
    plt.subplot(3, 2, 1)
    plt.hist(HY[prompt_type].cpu().numpy(), bins=30, histtype="step", label="low_e_high_a")
    plt.hist(HY[~prompt_type].cpu().numpy(), bins=30, histtype="step", label="high_e_low_a")
    plt.xlabel("H(Y)")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(3, 2, 2)
    plt.hist(HYX[prompt_type].cpu().numpy(), bins=30, histtype="step", label="low_e_high_a")
    plt.hist(HYX[~prompt_type].cpu().numpy(), bins=30, histtype="step", label="high_e_low_a")
    plt.xlabel("H(Y|X)")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(3, 2, 3)
    plt.hist(MI[prompt_type].cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist(MI[~prompt_type].cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("Mutual Information I(X;Y)")
    plt.ylabel("Count")
    plt.legend()


    plt.subplot(3, 2, 4)
    plt.hist(MI[prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist(MI[~prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("Log(Mutual Information)")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(3, 2, 5)
    plt.hist((MI/HY)[prompt_type].cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist((MI/HY)[~prompt_type].cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("Normalized Mutual Information I(X;Y) / H(Y)")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(3, 2, 6)
    plt.hist((MI/HY)[prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="low_e_high_a")
    plt.hist((MI/HY)[~prompt_type].log().cpu().numpy(), bins=50, histtype="step", label="high_e_low_a")
    plt.xlabel("log(Normalized Mutual Information)")
    plt.ylabel("Count")
    plt.legend()

    plt.savefig(f"{img_output_dir}/mutual_info.png", bbox_inches='tight')
    
    print("--- MI classifier ---")
    simple_classification(MI.to(DEVICE), prompt_type.to(DEVICE))

    print("--- log(MI) classifier ---")
    simple_classification((MI).log().to(DEVICE), prompt_type.to(DEVICE))

    print("--- MI_normal classifier ---")
    simple_classification((MI/HY).to(DEVICE), prompt_type.to(DEVICE))

    print("--- log(MI_normal) classifier ---")
    simple_classification((MI/HY).log().to(DEVICE), prompt_type.to(DEVICE))

    return HYX, HY, MI 


def plot_entropy(original_entropy, new_entropy, large_entropy, prompt_type):

    plt.figure()
    plt.hist(new_entropy[prompt_type].flatten(), bins=50, histtype="step", label="ALL(new low_e_high_a) - orig")
    plt.hist(new_entropy[~prompt_type].flatten(), bins=50, histtype="step", label="ALL(new high_e_low_a) - orig")
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig(f"{img_output_dir}/new_entropy.png", bbox_inches='tight')

    plt.figure(figsize=(12, 12))
    plt.subplot(2, 2, 1)
    plt.hist(original_entropy[prompt_type].flatten() - new_entropy[prompt_type].min(axis=1), bins=50, histtype="step", label="MIN(new low_e_high_a)  - orig")
    plt.hist(original_entropy[~prompt_type].flatten() - new_entropy[~prompt_type].min(axis=1), bins=50, histtype="step", label="MIN(new high_e_low_a) - orig")
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.title("A")
    plt.ylabel("Count")
    plt.legend()

    plt.subplot(2, 2, 2)
    plt.hist(original_entropy[prompt_type].flatten() - new_entropy[prompt_type].min(axis=1), bins=50, histtype="step", label="MIN(new low_e_high_a)  - orig")
    plt.hist(original_entropy[~prompt_type].flatten() - new_entropy[~prompt_type].max(axis=1), bins=50, histtype="step", label="MAX(new high_e_low_a  - orig)")
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.ylabel("Count")
    plt.title("B")
    plt.legend()


    plt.subplot(2, 2, 3)
    plt.hist(original_entropy[prompt_type].flatten() - new_entropy[prompt_type].max(axis=1), bins=50, histtype="step", label="MAX(new low_e_high_a) - orig")
    plt.hist(original_entropy[~prompt_type].flatten() - new_entropy[~prompt_type].min(axis=1), bins=50, histtype="step", label="MIN(new high_e_low_a) - orig")
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.ylabel("Count")
    plt.title("C")
    plt.legend()

    plt.subplot(2, 2, 4)
    plt.hist(original_entropy[prompt_type].flatten() - new_entropy[prompt_type].max(axis=1), bins=50, histtype="step", label="MAX(new low_e_high_a)")
    plt.hist(original_entropy[~prompt_type].flatten() - new_entropy[~prompt_type].max(axis=1), bins=50, histtype="step", label="MAX(new high_e_low_a)")
    plt.hist(original_entropy[prompt_type].flatten(), bins=30, histtype="step", label="original low_e_high_a", alpha=0.5)
    plt.hist(original_entropy[~prompt_type].flatten(), bins=30, histtype="step", label="original high_e_low_a", alpha=0.5)
    plt.xlabel("Entropy")
    plt.ylabel("Count")
    plt.title("D")
    plt.legend()

    plt.savefig(f"{img_output_dir}/new_entropy_breakdown.png", bbox_inches='tight')


    plt.figure()
    plt.hist(large_entropy, bins=30, histtype="step", label="Large Model Original Entropy")
    plt.hist(original_entropy[prompt_type], bins=30, histtype="step", label="Small Model Original Entropy low_e_high_a")
    plt.hist(original_entropy[~prompt_type], bins=30, histtype="step", label="Small Model Original Entropy shigh_e_low_a")
    plt.xlabel("Original Entropy")
    plt.ylabel("Count")
    plt.legend()
    plt.savefig("results/original_entropy.png")

    print("--- ALL(entropy) classifier ---")
    simple_classification(torch.from_numpy(new_entropy).to(DEVICE), torch.from_numpy(prompt_type).to(DEVICE))


def simple_classification(x_train, y_train):
    if len(x_train.shape) == 1:
        x_train = x_train.reshape(-1, 1)
    class LinearClassifier(torch.nn.Module):
        def __init__(self, input_dim=1, output_dim=1):
            super(LinearClassifier, self).__init__()
            self.linear = torch.nn.Linear(input_dim, output_dim)

        def forward(self, x):
            x = self.linear(x)
            return x
        
    model = LinearClassifier(input_dim=x_train.shape[1])
    model.to(DEVICE)
    criterion = torch.nn.BCEWithLogitsLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.2)

    all_loss = []
    for i in range(1001):
        output = model(x_train)

        loss = criterion(output.view(-1), y_train.float())
        all_loss.append(loss.item())
        loss.backward()

        optimizer.step()
        optimizer.zero_grad()

        if i % 200 == 0: 
            prediction = output.view(-1) > 0
            accuracy = torch.sum((prediction==y_train).bool()) / y_train.shape[0]
            print(f"\tEpoch {i}, Loss:{loss.item():.3f}, Acc:{accuracy.item():.2f}")


    
if __name__ == "__main__":

    from jsonargparse import CLI

    CLI(main)
