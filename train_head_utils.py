import logging
import os
import pickle
import random
import time

import torch
import torch.nn as nn


class PrecomputedShardLoader:
    def __init__(self, 
        shard_dirs: list[str],
    ):
        """
            Loads shards generated by precompute_logits.py and yields examples one by one.

            Args:
                shard_dirs: 
                    List of directories containing (only) sets of 
                    corresponding shards computed by precompute_logits.py
                shuffle_shards: 
                    Whether to shuffle the shards
                shuffle_seed: 
                    Seed for shuffling the shards
        """
        self.shard_dirs = shard_dirs

        shard_name_lists = []
        for shard_dir in self.shard_dirs:
            shards = os.listdir(shard_dir)

            # Shard names are assumed to be in the format "name_number.pickle"
            shards = list(sorted(shards, key=lambda x: int(x.split('_')[-1].strip('.pickle'))))

            shard_name_lists.append(shards)

        l = len(shard_name_lists[0])
        assert(all([len(shard_name_list) == l for shard_name_list in shard_name_lists]))

        shards = list(zip(*shard_name_lists))
        self.shards = shards

    def load_shards(self, shard_id: int):
        shards = []
        for shard_dir, shard_name in zip(self.shard_dirs, self.shards[shard_id]):
            shard_path = os.path.join(shard_dir, shard_name)
            with open(shard_path, "rb") as fp:
                shard = pickle.load(fp)

            shards.append(shard)

        return shards

    def shuffle_shards(self, seed: int):
        random.Random(seed).shuffle(self.shards)

    def __iter__(self):
        return self._gen()

    def _gen(self):
        """
            Returns a generator that yields tuples of examples one by one.
        """
        cur_shard_id = 0
        while cur_shard_id < len(self.shards):
            t = time.time()
            # Load corresponding shards
            t = time.time()
            logging.info(f"Loading shards...")
            loaded_shards = self.load_shards(cur_shard_id)
            logging.info(f"Shards loaded ({time.time() - t:.02f} seconds)...")

            # All shards in the tuple should be of the same length
            shard_len = len(loaded_shards[0])
            assert(all([len(shard) == shard_len for shard in loaded_shards]))

            # Sort examples within each shard by key
            sort_shard = lambda l: list(sorted(l.items(), key=lambda t: t[0]))
            for i in range(len(loaded_shards)):
                loaded_shards[i] = sort_shard(loaded_shards[i])
            
            yield from zip(*loaded_shards)

            cur_shard_id += 1

            del loaded_shards


def load_lm_head(checkpoint_path: str, dtype: torch.dtype, device: str):
    # Load the small model's LM head
    logging.info(f"Loading model at {checkpoint_path}... ")
    t = time.time()
    checkpoint = torch.load(checkpoint_path)
    assert(len([k for k in checkpoint.keys() if "lm_head" in k]) == 1)
    lm_head_weights = checkpoint["lm_head.weight"]
    vocab_size, emb_dim = lm_head_weights.shape
    lm_head = nn.Linear(
        emb_dim, vocab_size, bias=False
    )
    with torch.no_grad():
        lm_head.weight.data = lm_head_weights.to(dtype)
        lm_head.eval()
        lm_head = lm_head.to(device)

    logging.info(f"Time: {time.time() - t:.02f} seconds.")

    del checkpoint

    return lm_head
