import copy
import json
import logging
import os
from pathlib import Path
import pickle
import random
import sys
import time
from typing import Optional, Iterator, Tuple
import warnings

import lightning as L
import torch
import torch.nn as nn

from lit_llama import LLaMA, Tokenizer
from lit_llama.model import pipeLLaMA, LLaMAConfig
from lit_llama.utils import EmptyInitOnDevice, jsd


DTYPE = torch.float32
DEVICE = torch.device("cuda:0")


class DistancePredictionHead(nn.Module):
    def __init__(self,
        input_dim: int,
        no_bins: int,
        hidden_dim: int,
        no_hidden_layers: int,
        dropout: float,
        log_scale: bool = True,
        activation: str = "relu",
    ):
        super().__init__()
        self.input_dim = input_dim
        self.no_bins = no_bins
        self.hidden_dim = hidden_dim
        self.no_hidden_layers = no_hidden_layers
        self.dropout = dropout
        self.log_scale = log_scale

        if activation == "relu":
            activation_class = nn.ReLU
        else:
            raise ValueError(f"Unknown activation: {activation}")

        self.layers = nn.ModuleList()
        self.layers.append(nn.Linear(input_dim, hidden_dim))
        self.layers.append(nn.Dropout(dropout))
        self.layers.append(activation_class())
        for _ in range(no_hidden_layers):
            self.layers.append(nn.Linear(hidden_dim, hidden_dim))
            self.layers.append(nn.Dropout(dropout))
            self.layers.append(activation_class())

        self.layers.append(nn.Linear(hidden_dim, no_bins))

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)

        return torch.nn.functional.softmax(x, dim=-1)


class PrecomputedShardLoader:
    def __init__(self, 
        shard_dirs: list[str],
    ):
        """
            Loads shards generated by precompute_logits.py and yields examples one by one.

            Args:
                shard_dirs: 
                    List of directories containing (only) sets of 
                    corresponding shards computed by precompute_logits.py
                shuffle_shards: 
                    Whether to shuffle the shards
                shuffle_seed: 
                    Seed for shuffling the shards
        """
        self.shard_dirs = shard_dirs

        shard_name_lists = []
        for shard_dir in self.shard_dirs:
            shards = os.listdir(shard_dir)

            # Shard names are assumed to be in the format "name_number.pickle"
            shards = list(sorted(shards, key=lambda x: int(x.split('_')[-1].strip('.pickle'))))

            shard_name_lists.append(shards)

        l = len(shard_name_lists[0])
        assert(all([len(shard_name_list) == l for shard_name_list in shard_name_lists]))

        shards = list(zip(*shard_name_lists))
        self.shards = shards

    def load_shards(self, shard_id: int):
        shards = []
        for shard_dir, shard_name in zip(self.shard_dirs, self.shards[shard_id]):
            shard_path = os.path.join(shard_dir, shard_name)
            with open(shard_path, "rb") as fp:
                shard = pickle.load(fp)

            shards.append(shard)

        return shards

    def shuffle_shards(self, seed: int):
        random.Random(seed).shuffle(self.shards)

    def __iter__(self):
        return self._gen()

    def _gen(self):
        """
            Returns a generator that yields tuples of examples one by one.
        """
        cur_shard_id = 0
        while cur_shard_id < len(self.shards):
            t = time.time()
            # Load corresponding shards
            t = time.time()
            logging.info(f"Loading shards...")
            loaded_shards = self.load_shards(cur_shard_id)
            logging.info(f"Shards loaded ({time.time() - t:.02f} seconds)...")

            # All shards in the tuple should be of the same length
            shard_len = len(loaded_shards[0])
            assert(all([len(shard) == shard_len for shard in loaded_shards]))

            # Sort examples within each shard by key
            sort_shard = lambda l: list(sorted(l.items(), key=lambda t: t[0]))
            for i in range(len(loaded_shards)):
                loaded_shards[i] = sort_shard(loaded_shards[i])
            
            yield from zip(*loaded_shards)

            cur_shard_id += 1

            del loaded_shards


def load_lm_head(checkpoint_path: str):
    # Load the small model's LM head
    logging.info(f"Loading model at {checkpoint_path}... ")
    t = time.time()
    checkpoint = torch.load(checkpoint_path)
    assert(len([k for k in checkpoint.keys() if "lm_head" in k]) == 1)
    lm_head_weights = checkpoint["lm_head.weight"]
    vocab_size, emb_dim = lm_head_weights.shape
    lm_head = nn.Linear(
        emb_dim, vocab_size, bias=False
    )
    with torch.no_grad():
        lm_head.weight.data = lm_head_weights.to(DTYPE)
        lm_head.eval()
        lm_head = lm_head.to(DEVICE)

    logging.info(f"Time: {time.time() - t:.02f} seconds.")

    del checkpoint

    return lm_head


def _discretize(
    values: torch.Tensor, 
    no_bins: int, 
    mi: float, 
    ma: float
):
    """
        Discretizes the target into `no_bins` bins.
    """
    assert(mi < ma)
    assert(no_bins > 0)

    boundaries = torch.linspace(
        mi, ma, no_bins + 1, device=values.device
    )

    # Make shapes compatible
    boundaries = boundaries.view(*([1]*len(values.shape)), -1)
    values = values.unsqueeze(-1)

    lt = boundaries[..., :-1] <= values
    gt = boundaries[..., 1:] > values
    bin_id = torch.logical_and(lt, gt).to(torch.int64).argmax(dim=-1)
    
    return bin_id


def _preprocessor(
    shard_loader: PrecomputedShardLoader,
    small_lm_head: nn.Linear,
    large_lm_head: nn.Linear,
    no_bins: int,
    min_bin: float,
    max_bin: float,
    device: torch.device,
):
    for i, (small_tup, large_tup) in enumerate(shard_loader):
        small_key, small_emb = small_tup
        large_key, large_emb = large_tup

        # Sanity check. The shards should be aligned such that the keys match.
        assert(small_key == large_key)

        # Some empty articles slipped through my filter. Sad!
        if(small_emb.shape[0] == 1):
            continue

        small_emb = small_emb.to(device=device, dtype=DTYPE)
        large_emb = large_emb.to(device=device, dtype=DTYPE)
        
        with torch.no_grad():
            # Compute logits from the small model embeddings
            small_logits = small_lm_head(small_emb)
            large_logits = large_lm_head(large_emb)

            # Softmax both sets of logits
            small_logits = torch.nn.functional.softmax(small_logits, dim=-1)
            large_logits = torch.nn.functional.softmax(large_logits, dim=-1)

            # Compute the JS divergence between the two distributions
            divergence = jsd(small_logits, large_logits)
            
            # We will predict the log of the divergence
            target = torch.log(divergence)

            # Discretize the target
            target = _discretize(
                target,
                no_bins, 
                mi=min_bin, 
                ma=max_bin,
            ).squeeze(0)

        yield (small_emb, target)


def batch_loader(
    data_gen: Iterator[Tuple[torch.Tensor, torch.Tensor]],
    batch_size: int,
    skip_frac: float,
):
    batch = []
    for i, (small_emb, log_js_div) in enumerate(data_gen):
        # [N, emb_dim]
        assert(len(small_emb.shape) == 2)
        inputs = torch.unbind(small_emb, dim=-2)

        # [N]
        assert(len(log_js_div.shape) == 1)
        targets = torch.unbind(log_js_div, dim=-1)

        assert(len(inputs) == len(targets))

        for inp, target in zip(inputs, targets):
            # We don't want too many consecutive tokens from the same prompt,
            # so we skip a large percentage of them.
            if(random.random() < skip_frac):
                continue

            batch.append((inp, target))

            if(len(batch) == batch_size):
                inputs = torch.stack([t[0] for t in batch])
                targets = torch.stack([t[1] for t in batch])

                assert(inputs.device == targets.device)

                yield inputs, targets

                batch = []

    # Toss the last batch if it's too small
    pass


def main(
    *,
    precomputed_small_emb_dir: str,
    precomputed_large_emb_dir: str,
    output_dir: str,
    model_size: str = "7B",
    small_checkpoint_path: str = None,
    large_checkpoint_path: str = None,
    hidden_dim: int = 1024,
    no_hidden_layers: int = 4,
    dropout: float = 0.1,
    activation: str = "relu",
    lr: float = 1e-5,
    batch_size: int = 128,
    no_epochs: int = 10,
    skip_frac: float = 0.95,
    no_bins: int = 25,
    min_bin: float = -3.,
    max_bin: float = 0.,
    seed: int = 42,
    precomputed_small_emb_dir_val: str = None,
    precomputed_large_emb_dir_val: str = None,
    eval_every_n_batches: int = 1000,
) -> None:
    """
    Args:
        precomputed_small_emb_dir: Directory containing embeddings for the small model (generated by precompute_logits.py)
        precomputed_large_emb_dir: Directory containing embeddings for the large model (generated by precompute_logits.py)
        output_dir: Where to save output files
        model_size: The size of the SMALL model. E.g. "7B" or "30B"
        checkpoint_path: The small LM checkpoint path.
        hidden_dim: Hidden dimension of the distance prediction head.
        no_hidden_layers: Number of hidden layers in the distance prediction head.
        dropout: Dropout probability in the distance prediction head.
        activation: Activation function in the distance prediction head.
        lr: Learning rate.
        batch_size: Batch size.
        no_epochs: Number of epochs.
        skip_frac: Probability of skipping any given token.
        no_bins: Number of bins to discretize the target into.
        min_bin: Minimum value of the discretized target.
        max_bin: Maximum value of the discretized target.
    """
    torch.manual_seed(seed)
    random.seed(seed)

    if not small_checkpoint_path:
        small_checkpoint_path = Path(f'/n/holystore01/LABS/barak_lab/Everyone/checkpoints/checkpoints/lit-llama/7B/state_dict.pth')
    if not large_checkpoint_path:
        large_checkpoint_path = Path(f'/n/holystore01/LABS/barak_lab/Everyone/checkpoints/checkpoints/lit-llama/30B/state_dict.pth')

    assert small_checkpoint_path.is_file()
    assert large_checkpoint_path.is_file()

    small_lm_head = load_lm_head(small_checkpoint_path)
    large_lm_head = load_lm_head(large_checkpoint_path)

    # Load the precomputed logits
    logit_loader = PrecomputedShardLoader(
        [
            precomputed_small_emb_dir,
            precomputed_large_emb_dir
        ],
    )

    val = precomputed_small_emb_dir_val is not None
    if(val and precomputed_large_emb_dir_val is None):
        raise ValueError("Must provide both small and large validation directories")

    if(val):
        logging.info("Validation enabled...")
    else:
        logging.warning("Validation disabled...")

    val_logit_loader = None
    if(val):
        val_logit_loader = PrecomputedShardLoader(
            [
                precomputed_small_emb_dir_val,
                precomputed_large_emb_dir_val,
            ],
        )

    # Initialize the model
    distance_prediction_head = DistancePredictionHead(
        input_dim=small_lm_head.weight.shape[1],
        no_bins=no_bins,
        hidden_dim=hidden_dim,
        no_hidden_layers=no_hidden_layers,
        dropout=dropout,
        activation=activation,
    )
    distance_prediction_head.to(DEVICE)
    param_count = sum(
        p.numel() for p in distance_prediction_head.parameters() if p.requires_grad
    )
    logging.info(f"Loaded prediction head ({param_count} parameters)...")

    # Umm uhh
    distance_prediction_head = torch.compile(distance_prediction_head)

    # Initialize the optimizer
    optimizer = torch.optim.Adam(
        distance_prediction_head.parameters(), 
        lr=lr
    )

    # Select the loss function
    loss_fn = torch.nn.functional.cross_entropy

    # Standard training loop
    for epoch in range(no_epochs):
        # Run the training loop
        shuffle_seed = random.randint(0, 2**32 - 1)
        logit_loader.shuffle_shards(shuffle_seed)

        data_gen = _preprocessor(
            shard_loader=logit_loader,
            small_lm_head=small_lm_head,
            large_lm_head=large_lm_head,
            no_bins=no_bins,
            min_bin=min_bin,
            max_bin=max_bin,
            device=DEVICE,
        )

        bl = batch_loader(
            data_gen=data_gen,
            batch_size=batch_size,
            skip_frac=skip_frac,
        )

        for i, (inputs, targets) in enumerate(bl):
            # Periodically run the validation loop
            if(val and i % eval_every_n_batches == 0):
                val_data_gen = _preprocessor(
                    shard_loader=val_logit_loader,
                    small_lm_head=small_lm_head,
                    large_lm_head=large_lm_head,
                    no_bins=no_bins,
                    min_bin=min_bin,
                    max_bin=max_bin,
                    device=DEVICE,
                )

                val_bl = batch_loader(
                    data_gen=val_data_gen,
                    batch_size=batch_size,
                    skip_frac=0.,
                )

                with torch.no_grad():
                    loss_sum = 0
                    count = 0
                    for i, (inputs, targets) in enumerate(val_bl):
                        inputs = inputs.to(DEVICE)
                        targets = targets.to(DEVICE)

                        inputs = inputs.to(torch.float32)
                        targets = targets.to(torch.int64)

                        outputs = distance_prediction_head(inputs)
                        loss = loss_fn(outputs, targets)
                        loss_sum += torch.sum(loss).item()

                        count += loss.numel()

                    print(f"Validation loss: {loss_sum / count}")

            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)

            inputs = inputs.to(torch.float32)
            targets = targets.to(torch.int64)

            optimizer.zero_grad()
            outputs = distance_prediction_head(inputs)
            loss = loss_fn(outputs, targets)
            loss.backward()
            optimizer.step()

            if i % 100 == 0:
                print(f"Epoch {epoch}, batch {i}, loss {loss.item():.02f}", file=sys.stderr)

                


    # Save the model
    os.path.makedirs(output_dir, exist_ok=True)
    model_path = os.path.join(output_dir, "state_dict.pth")
    torch.save(distance_prediction_head.state_dict(), model_path)


if __name__ == "__main__":
    from jsonargparse import CLI

    torch.set_float32_matmul_precision("high")
    warnings.filterwarnings(
        # Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31
        "ignore", 
        message="ComplexHalf support is experimental and many operators don't support it yet"
    )
    warnings.filterwarnings(
        # Triggered in bitsandbytes/autograd/_functions.py:298
        "ignore", 
        message="MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization",
    )
    warnings.filterwarnings(
        # SLURM srun warning
        "ignore", 
        message="The `srun` command is available on your system but is not used",
    )

    CLI(main)