import copy
import json
import logging
import os
from pathlib import Path
import pickle
import random
import sys
import time
from typing import Optional
import warnings

import lightning as L
import torch
import torch.nn as nn

from lit_llama import LLaMA, Tokenizer
from lit_llama.model import pipeLLaMA, LLaMAConfig
from lit_llama.utils import EmptyInitOnDevice, jsd


DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32

assert(torch.cuda.device_count() >= 2)
DEVICE = torch.device("cuda:0")


class DistancePredictionHead(nn.Module):
    def __init__(self,
        input_dim: int,
        no_bins: int,
        hidden_dim: int,
        no_hidden_layers: int,
        dropout: float,
        log_scale: bool = True,
        activation: str = "relu",
    ):
        super().__init__()
        self.input_dim = input_dim
        self.no_bins = no_bins
        self.hidden_dim = hidden_dim
        self.no_hidden_layers = no_hidden_layers
        self.dropout = dropout
        self.log_scale = log_scale

        if activation == "relu":
            activation_class = nn.ReLU
        else:
            raise ValueError(f"Unknown activation: {activation}")

        self.layers = nn.ModuleList()
        self.layers.append(nn.Linear(input_dim, hidden_dim))
        self.layers.append(nn.Dropout(dropout))
        self.layers.append(activation_class())
        for _ in range(no_hidden_layers):
            self.layers.append(nn.Linear(hidden_dim, hidden_dim))
            self.layers.append(nn.Dropout(dropout))
            self.layers.append(activation_class())

        self.layers.append(nn.Linear(hidden_dim, no_bins))

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)

        return torch.nn.functional.softmax(x, dim=-1)


class PrecomputedShardLoader:
    def __init__(self, 
        shard_dirs: list[str],
    ):
        """
            Loads shards generated by precompute_logits.py and yields examples one by one.

            Args:
                shard_dirs: 
                    List of directories containing (only) sets of 
                    corresponding shards computed by precompute_logits.py
                shuffle_shards: 
                    Whether to shuffle the shards
                shuffle_seed: 
                    Seed for shuffling the shards
        """
        self.shard_dirs = shard_dirs

        shard_name_lists = []
        for shard_dir in self.shard_dirs:
            shards = os.listdir(shard_dir)

            # Shard names are assumed to be in the format "name_number.pickle"
            shards = list(sorted(shards, key=lambda x: int(x.split('_')[-1].strip('.pickle'))))

            shard_name_lists.append(shards)

        l = len(shard_name_lists[0])
        assert(all([len(shard_name_list) == l for shard_name_list in shard_name_lists]))

        shards = list(zip(*shard_name_lists))
        self.shards = shards

    def load_shards(self, shard_id: int):
        shards = []
        for shard_dir, shard_name in zip(self.shard_dirs, self.shards[shard_id]):
            shard_path = os.path.join(shard_dir, shard_name)
            with open(shard_path, "rb") as fp:
                shard = pickle.load(fp)

            shards.append(shard)

        return shards

    def shuffle_shards(self, seed: int):
        random.Random(seed).shuffle(self.shards)

    def gen(self):
        """
            Returns a generator that yields tuples of examples one by one.
        """
        cur_shard_id = 0
        while cur_shard_id < len(self.shards):
            t = time.time()
            # Load corresponding shards
            t = time.time()
            logging.info(f"Loading shards...")
            loaded_shards = self.load_shards(cur_shard_id)
            logging.info(f"Shards loaded ({time.time() - t:.02f} seconds)...")

            # All shards in the tuple should be of the same length
            shard_len = len(loaded_shards[0])
            assert(all([len(shard) == shard_len for shard in loaded_shards]))

            # Sort examples within each shard by key
            sort_shard = lambda l: list(sorted(l.items(), key=lambda t: t[0]))
            for i in range(len(loaded_shards)):
                loaded_shards[i] = sort_shard(loaded_shards[i])
            
            yield from zip(*loaded_shards)

            cur_shard_id += 1

            del loaded_shards


def _discretize(
    value: torch.Tensor, 
    no_bins: int, 
    minimum: float, 
    maximum: float
):
    """
        Discretizes the target into `no_bins` bins.
    """
    assert(min < max)
    assert(no_bins > 0)
    assert(len(value.shape) == 2)

    boundaries = torch.linspace(
        minimum, maximum, no_bins + 1, device=value.device
    )
    bins = bins.unsqueeze(0)

    lt = boundaries[:-1] <= value
    gt = boundaries[1:] > value
    bin_id = torch.logical_and(lt, gt).argmax(dim=-1)
    
    return torch.nn.functional.one_hot(bin_id, no_bins)


def _unpack_shard_tups(small_tup, large_tup, small_lm_head):
    small_key, small_emb = small_tup
    large_key, large_logits = large_tup

    # Sanity check. The shards should be aligned such that the keys match.
    assert(small_key == large_key)
    
    with torch.no_grad():
        # Compute logits from the small model embeddings
        small_logits = small_lm_head(small_emb)

        # Softmax both sets of logits
        small_logits = torch.nn.functional.softmax(small_logits, dim=-1)
        large_logits = torch.nn.functional.softmax(large_logits, dim=-1)

        # Compute the JS divergence between the two distributions
        distance = jsd(small_logits, large_logits)
        
        # We will predict the log of the distance
        target = torch.log(js_div)

    return (small_logits, distance)


def batch_loader(
    batch_size: int,
    shard_loader: PrecomputedShardLoader,
    lm_head: nn.Linear, 
    skip_frac: float, 
    no_bins: int,
    device: torch.device,
):
    data_gen = shard_loader.gen()
    batch = []
    for i, (small_tup, large_tup) in enumerate(data_gen):
        small_emb, log_js_div = _unpack_shard_tups(
            small_tup, large_tup, lm_head
        )

        small_emb = small_emb.to(device)
        log_js_div = log_js_div.to(device)

        # [N, emb_dim]
        assert(len(small_emb.shape) == 2)
        inputs = torch.unbind(small_emb, dim=-2)

        # [N]
        assert(len(target.shape) == 1)
        targets = torch.unbind(target, dim=-1)

        assert(len(inputs) == len(targets))

        for inp, target in zip(inputs, targets):
            if(random.random() < skip_frac):
                continue
            batch.append((inp, target))

            if(len(batch) == batch_size):
                inputs = torch.stack([t[0] for t in batch])
                targets = torch.stack([t[1] for t in batch])
        
                # Discretize the targets
                minimum = -3. # log(1e-3)
                maximum = 0. # log(1)
                target = _discretize_target(
                    targets,
                    no_bins, 
                    minimum=minimum, 
                    maximum=maximum,
                )

                yield inputs, targets



def main(
    *,
    precomputed_small_emb_dir: str,
    precomputed_large_logit_dir: str,
    output_dir: str,
    model_size: str = "7B",
    checkpoint_path: str = None,
    quantize: Optional[str] = None,
    no_bins: int = 100,
    hidden_dim: int = 1024,
    no_hidden_layers: int = 2,
    dropout: float = 0.1,
    activation: str = "relu",
    lr: float = 1e-4,
    batch_size: int = 128,
    no_epochs: int = 10,
    skip_frac: float = 0.98,
) -> None:
    """
    Args:
        precomputed_small_emb_dir: Directory containing embeddings for the small model (generated by precompute_logits.py)
        precomputed_large_logit_dir: Directory containing logits for the large model (generated by precompute_logits.py)
        output_dir: Where to save output files
        model_size: The size of the SMALL model. E.g. "7B" or "30B"
        checkpoint_path: The small LM checkpoint path.
        quantize: Whether to quantize the model and using which method:
            ``"llm.int8"``: LLM.int8() mode,
            ``"gptq.int4"``: GPTQ 4-bit mode.
    """
    if not checkpoint_path:
        checkpoint_path = Path(f'/n/holystore01/LABS/barak_lab/Everyone/checkpoints/checkpoints/lit-llama/{model_size}/state_dict.pth')
    
    assert checkpoint_path.is_file()

    # Load the small model's LM head
    with EmptyInitOnDevice(
        device=DEVICE, dtype=DTYPE, quantization_mode=quantize
    ):
        print("Loading small model... ", file=sys.stderr, end='')
        t0 = time.time()
        checkpoint = torch.load(checkpoint_path)
        assert(len([k for k in checkpoint.keys() if "lm_head" in k]) == 1)
        lm_head_weights = checkpoint["lm_head.weight"]
        vocab_size, emb_dim = lm_head_weights.shape
        lm_head = nn.Linear(
            emb_dim, vocab_size, bias=False
        )
        with torch.no_grad():
            lm_head.weight.data = lm_head_weights
            lm_head.eval()

        print(f"Time: {time.time() - t0:.02f} seconds.", file=sys.stderr)

        del checkpoint

    # Load the precomputed logits
    logit_loader = PrecomputedShardLoader(
        [
            precomputed_small_emb_dir,
            precomputed_large_logit_dir
        ],
    )

    # Initialize the model
    distance_prediction_head = DistancePredictionHead(
        input_dim=emb_dim,
        no_bins=no_bins,
        hidden_dim=hidden_dim,
        no_hidden_layers=no_hidden_layers,
        dropout=dropout,
        activation=activation,
    )
    distance_prediction_head.to(DEVICE)

    optimizer = torch.optim.Adam(
        distance_prediction_head.parameters(), 
        lr=lr
    )

    loss_fn = torch.nn.functional.cross_entropy

    for epoch in range(no_epochs):
        shuffle_seed = random.randint(0, 2**32 - 1)
        logit_loader.shuffle_shards(shuffle_seed)
        bl = batch_loader(
            batch_size=batch_size,
            shard_loader=logit_loader,
            lm_head=lm_head,
            skip_frac=skip_frac,
            no_bins=no_bins,
        )

        for i, (inputs, targets) in enumerate(bl):
            inputs = inputs.to(DEVICE)
            targets = targets.to(DEVICE)

            optimizer.zero_grad()
            outputs = distance_prediction_head(inputs)
            loss = loss_fn(outputs, targets)
            loss.backward()
            optimizer.step()

            if i % 100 == 0:
                print(f"Epoch {epoch}, batch {i}, loss {loss.item():.02f}", file=sys.stderr)


    # Save the model
    os.path.makedirs(output_dir, exist_ok=True)
    model_path = os.path.join(output_dir, "state_dict.pth")
    torch.save(distance_prediction_head.state_dict(), model_path)


if __name__ == "__main__":
    from jsonargparse import CLI

    torch.set_float32_matmul_precision("high")
    warnings.filterwarnings(
        # Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31
        "ignore", 
        message="ComplexHalf support is experimental and many operators don't support it yet"
    )
    warnings.filterwarnings(
        # Triggered in bitsandbytes/autograd/_functions.py:298
        "ignore", 
        message="MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization",
    )
    warnings.filterwarnings(
        # SLURM srun warning
        "ignore", 
        message="The `srun` command is available on your system but is not used",
    )

    CLI(main)
