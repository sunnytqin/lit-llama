import copy
import json
import logging
import os
from pathlib import Path
import pickle
import random
import sys
import time
import warnings

import lightning as L
import numpy as np
import torch
import torch.nn as nn
# import wandb

from lit_llama import LLaMA, Tokenizer
from train_head_utils import (
    batch_loader,
    load_lm_head,
    load_llama_tokenizer,
    load_pythia_tokenizer,
    PrecomputedShardLoader,
    MAX_LEN,
    _preprocessor,
)

"""
This script creates a filter for a joint embedding dataset of the kind produced by precompute_logits.py.
Specifically, it identifies a subset of the data such that the small model entropy is in a given range
and then subsamples from this subset until the proportion of examples with low large model entropy is
approximately equal to the proportion of examples with high large model entropy.
"""


# DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32
DTYPE = torch.float32
DEVICE = torch.device("cuda:0")
DEFAULT_MODEL_DIRS = {
    "llama": "/n/holystore01/LABS/barak_lab/Everyone/checkpoints/checkpoints/lit-llama",
    "pythia": "/n/holystore01/LABS/barak_lab/Everyone/pythia",
}


def main(
    *,
    precomputed_small_emb_dir: str,
    precomputed_large_emb_dir: str,
    output_dir: str,
    dataset_json_path: str,
    model_type: str = "llama",
    small_model_size: str = "7B",
    large_model_size: str = "30B",
    small_checkpoint_path: str = None,
    large_checkpoint_path: str = None,
    tokenizer_path: str = None,
    entropy_min: float = 2.0,
    entropy_max: float = -1,
    entropy_delta: float = 0.1, 
    zero_entropy_threshold: float = 0.2,
    balanced_classes: bool = True,
    shard_output: bool = False,
    shard_size: int = 200, 
    seed: int = 42,
) -> None:
    """
    Args:
        precomputed_small_emb_dir: Directory containing embeddings for the small model (generated by precompute_logits.py)
        precomputed_large_emb_dir: Directory containing embeddings for the large model (generated by precompute_logits.py)
        output_dir: Where to save output files
        small_checkpoint_path: The small LM checkpoint path.
        large_checkpoint_path: The large LM checkpoint path.
        entropy_min: The lower bound of the entropy range.
        entropy_max: The upper bound of the entropy range (or -1).
        zero_entropy_threshold: The threshold at which the large model entropy is considered zero.
        balanced_classes: Whether to balance the classes by subsampling the larger class.
    """
    # MUST COME FIRST
    args = locals()

    torch.manual_seed(seed)

    if(entropy_max == -1):
        entropy_max = float("inf")

    # Make the output directory
    os.makedirs(output_dir, exist_ok=True)
    for nn in ['filter', 'large_entropy', 'small_entropy']:
        inner_dir = os.path.join(output_dir, nn)
        os.makedirs(inner_dir, exist_ok=True)

    if not small_checkpoint_path:
        default_model_dir = DEFAULT_MODEL_DIRS[model_type]
        if(model_type == "llama"):
            small_checkpoint_path = f"{default_model_dir}/{small_model_size}/lit-llama.pth"
        elif(model_type == "pythia"):
            small_checkpoint_path = f"{default_model_dir}/pythia-{small_model_size}/"
            # small_model_size = '1.4b'
        else:
            raise ValueError

    if not large_checkpoint_path:
        if(model_type == "llama"):
            large_checkpoint_path = f"{default_model_dir}/{large_model_size}/lit-llama.pth"
        elif(model_type == "pythia"):
            large_checkpoint_path = f"{default_model_dir}/pythia-{large_model_size}/"
            # large_model_size = '12b'
        else:
            raise ValueError

    if not tokenizer_path:
        if(model_type == "llama"):
            tokenizer_path = '/n/holystore01/LABS/barak_lab/Everyone/checkpoints/checkpoints/lit-llama/tokenizer.model'
        elif(model_type == "pythia"):
            pass # Not necessary

    # Load the (small) LM heads of both the small and the large model.
    # We've only cached the embeddings and not the (much larger) logits.
    small_lm_head = load_lm_head(
        small_checkpoint_path, dtype=DTYPE, device=DEVICE, model_type=model_type, model_size=small_model_size
    )
    large_lm_head = load_lm_head(
        large_checkpoint_path, dtype=DTYPE, device=DEVICE, model_type=model_type, model_size=large_model_size
    )

    # Load the precomputed logits
    shard_dirs = [
        precomputed_small_emb_dir,
        precomputed_large_emb_dir,
    ]

    logit_loader = PrecomputedShardLoader(shard_dirs)

    filt = {}
    by_label = {}
    small_entropy_dict = {}
    large_entropy_dict = {}
    for i, shard_tups in enumerate(logit_loader):
        if(i % 1000 == 0):
            print(i)
            
        small_tup, large_tup = shard_tups

        small_key, small_emb = small_tup
        large_key, large_emb = large_tup

        # Patches up a previous bug in precompute_logits.py
        if(len(small_emb.shape) == 1):
            small_emb = small_emb.unsqueeze(0)
        if(len(large_emb.shape) == 1):
            large_emb = large_emb.unsqueeze(0)

        # Sanity check. The shards should be aligned such that the keys match.
        keys = set([t[0] for t in shard_tups])
        assert(len(keys) == 1)

        small_emb = small_emb.to(device=DEVICE, dtype=DTYPE)
        large_emb = large_emb.to(device=DEVICE, dtype=DTYPE)

        with torch.no_grad():
            # Compute logits from the small model embeddings
            small_logits = small_lm_head(small_emb)
            large_logits = large_lm_head(large_emb)

            # Softmax both sets of logits
            small_logits_softmax = torch.nn.functional.softmax(small_logits, dim=-1)
            large_logits_softmax = torch.nn.functional.softmax(large_logits, dim=-1)

            # Compute entropy
            small_logs = torch.nn.functional.log_softmax(small_logits, dim=-1)
            small_entropy = torch.sum(-1 * small_logits_softmax * small_logs, dim=-1)
            large_logs = torch.nn.functional.log_softmax(large_logits, dim=-1)
            large_entropy = torch.sum(-1 * large_logits_softmax * large_logs, dim=-1)

            small_entropy_in_range = torch.logical_and(
                small_entropy >= entropy_min, 
                small_entropy < entropy_max,
            )

            large_entropy_in_range = torch.logical_and(
                large_entropy >= small_entropy - entropy_delta, 
                large_entropy <= small_entropy + entropy_delta,
            )

            large_entropy_zero = large_entropy < zero_entropy_threshold

            # e = epistemic, a = aleatoric
            high_e_low_a = torch.logical_and(
                small_entropy_in_range,
                large_entropy_zero,
            )

            low_e_high_a = torch.logical_and(
                small_entropy_in_range,
                large_entropy_in_range,
            )

            zero_dict = by_label.setdefault("0", {})
            ones_dict = by_label.setdefault("1", {})

            zero_dict[small_key] = high_e_low_a
            ones_dict[small_key] = low_e_high_a

            small_entropy_dict[small_key] = small_entropy
            large_entropy_dict[small_key] = large_entropy
            
    # Balance the classes
    if(balanced_classes):
        # First, we need to balance individual token counts between classes

        # Load the tokenizer
        if(model_type == "llama"):
            tokenizer = load_llama_tokenizer(tokenizer_path, device=DEVICE)
        elif(model_type == "pythia"):
            tokenizer = load_pythia_tokenizer(small_model_size, device=DEVICE)
        else:
            raise ValueError(f"Model type: {model_type}")

        # Load the dataset
        with open(dataset_json_path, "r") as f:
            dataset = json.load(f)

        # Tokenize the dataset
        print("Tokenizing dataset...")
        tic = time.time()
        dataset_tokens = {}
        for key, text in dataset.items():
            tokens = tokenizer(text)[:MAX_LEN]

            if(len(tokens) == 0):
                continue

            dataset_tokens[key] = tokens

        toc = time.time() - tic
        print(f"Tokenized dataset in {toc} seconds")

        print("Balancing token counts...")
        tic = time.time()

        # Count tokens in each class using the class filters
        max_token_id = 0
        token_counts = {label: {} for label in by_label}
        for k in by_label["0"]:
            filters = {
                label: by_label[label][k] for label in by_label
            }
            
            tokens = dataset_tokens[k]

            # Quick sanity check
            assert(all(
                tokens.shape == filters[label].shape for label in filters
            ))

            for i, token in enumerate(tokens.tolist()):
                if(token > max_token_id):
                    max_token_id = token

                token_str = str(token)
                for label in filters:
                    token_counts[label].setdefault(token_str, 0)
                    if(filters[label][i]):
                        token_counts[label][token_str] = token_counts[label][token_str] + 1

        # Compute resampling probabilities for each token in each class
        token_sample_ratios = {
            label: torch.ones(max_token_id + 1, dtype=DTYPE, device=DEVICE) for label in token_counts
        }
        for k in token_counts["0"]:
            counts = {
                label: token_counts[label][k] for label in token_counts
            }

            min_count_key = min(counts, key=counts.get)
            for label in token_counts:
                if(counts[label] != 0):
                    token_sample_ratios[label][int(k)] = counts[min_count_key] / counts[label]

        # Resample tokens
        for k, token_tensor in dataset_tokens.items():
            for label in by_label:
                if(k not in by_label[label]):
                    continue

                # Retrieve token-specific sampling probabilities
                token_sample_ratio_tensor = token_sample_ratios[label]
                ratios_for_sequence = token_sample_ratio_tensor[token_tensor]

                # Sample a mask accordingly (0 = discard, 1 = keep)
                sample_mask = torch.rand(
                    ratios_for_sequence.shape, device=DEVICE,
                ) <= ratios_for_sequence
                
                # Apply it
                by_label[label][k] = torch.logical_and(
                    by_label[label][k],
                    sample_mask,
                )

        toc = time.time() - tic
        print(f"Balanced token counts in {toc} seconds")

        # # Validate token counts
        # max_token_id = 0
        # token_counts = {label: {} for label in by_label}
        # for k in by_label["0"]:
        #     filters = {
        #         label: by_label[label][k] for label in by_label
        #     }
            
        #     tokens = dataset_tokens[k]

        #     # Quick sanity check
        #     assert(all(
        #         len(tokens) == len(filters[label]) for label in filters
        #     ))

        #     for i, token in enumerate(tokens):
        #         token = token.item()

        #         if(token > max_token_id):
        #             max_token_id = token

        #         token_str = str(token)
        #         for label in filters:
        #             token_counts[label].setdefault(token_str, 0)
        #             if(filters[label][i]):
        #                 token_counts[label][token_str] = token_counts[label][token_str] + 1
        
        # print(list(sorted(token_counts["0"].items(), key=lambda x: x[1], reverse=True))[:100])
        # print(list(sorted(token_counts["1"].items(), key=lambda x: x[1], reverse=True))[:100])

        sizes = {
            label: sum([torch.sum(v) for v in by_label[label].values()]) 
            for label in by_label
        }

        print(f"Initial sizes: {sizes}")

        # Semi-vestigial code for balancing the number of tokens in each class
        # (in a token-agnostic way). The new token balancing code pretty much 
        # does this as a side effect, but I guess it's nice to clean up around
        # the edges a bit.
        max_class = max(sizes, key=sizes.get)
        min_class = min(sizes, key=sizes.get)

        fraction = sizes[min_class] / sizes[max_class]
        
        for key, f in by_label[max_class].items():
            subsample_mask = torch.rand(f.shape, device=f.device) <= fraction
            filtered = torch.logical_and(f, subsample_mask)
            filt[key] = torch.logical_or(
                by_label[min_class][key],
                filtered,
            )

            by_label[max_class][key] = filtered

        new_sizes = {
            label: sum([torch.sum(v) for v in by_label[label].values()]) 
            for label in by_label
        }

        print(f"New sizes: {new_sizes}")
        
    # Not balance the classes
    else:
        sizes = {
            "0": sum([torch.sum(v) for v in by_label["0"].values()]),
            "1": sum([torch.sum(v) for v in by_label["1"].values()]),
         }
        print(f"Initial sizes: {sizes}")
        
        max_class = max(sizes, key=sizes.get)
        min_class = min(sizes, key=sizes.get)

        fraction = sizes[min_class] / sizes[max_class]

        for key, f in by_label[max_class].items():
            subsample_mask = torch.rand(f.shape, device=f.device) <= fraction
            filtered = torch.logical_and(f, subsample_mask)
            filt[key] = torch.logical_or(
                by_label[min_class][key],
                filtered,
            )

            by_label[max_class][key] = filtered

        new_sizes = {
            "0": sum([torch.sum(v) for v in by_label["0"].values()]),
            "1": sum([torch.sum(v) for v in by_label["1"].values()]),
        }

        print(f"New sizes: {new_sizes}")

    if(shard_output):
        # split into smaller shards for repetition experiment
        filt_shard = {}
        filt_sum = 0 
        small_entropy_shard = {}
        large_entropy_shard = {}
        shard_count = 0
        for k,v in filt.items():
            if filt_sum >= shard_size: 
                print(f"shard {shard_count}, filter sum: {filt_sum}, {len(filt_shard)}")
    
                output_path = os.path.join(output_dir, "filter", f"filter_{shard_count}.pickle")
                with open(output_path, "wb") as fp:
                    pickle.dump(filt_shard, fp, protocol=pickle.HIGHEST_PROTOCOL)
    
                output_path = os.path.join(output_dir, "small_entropy", f"small_entropy_{shard_count}.pickle")
                with open(output_path, "wb") as fp:
                    pickle.dump(small_entropy_shard, fp, protocol=pickle.HIGHEST_PROTOCOL)
    
                output_path = os.path.join(output_dir, "large_entropy", f"large_entropy_{shard_count}.pickle")
                with open(output_path, "wb") as fp:
                    pickle.dump(large_entropy_shard, fp, protocol=pickle.HIGHEST_PROTOCOL)
                shard_count += 1
                filt_sum = 0
                filt_shard = {}
    
            filt_shard[k] = v.to(device="cpu")
            small_entropy_shard[k] = small_entropy_dict[k]
            large_entropy_shard[k] = large_entropy_dict[k]
            filt_sum += v.sum()
        
        # save the final batch 
        # shard_count += 1
        print(f"shard {shard_count}, filter sum: {filt_sum}, {len(filt_shard)}")
        output_path = os.path.join(output_dir, "filter", f"filter_{shard_count}.pickle")
        with open(output_path, "wb") as fp:
            pickle.dump(filt_shard, fp, protocol=pickle.HIGHEST_PROTOCOL)
    
        output_path = os.path.join(output_dir, "small_entropy", f"small_entropy_{shard_count}.pickle")
        with open(output_path, "wb") as fp:
            pickle.dump(small_entropy_shard, fp, protocol=pickle.HIGHEST_PROTOCOL)
    
        output_path = os.path.join(output_dir, "large_entropy", f"large_entropy_{shard_count}.pickle")
        with open(output_path, "wb") as fp:
            pickle.dump(large_entropy_shard, fp, protocol=pickle.HIGHEST_PROTOCOL)
    else:
        filt = {
            k: v.to(device="cpu") for k,v in filt.items()
        }

        output_path = os.path.join(output_dir, "filter.pickle")
        with open(output_path, "wb") as fp:
            pickle.dump(filt, fp, protocol=pickle.HIGHEST_PROTOCOL)


if __name__ == "__main__":
    from jsonargparse import CLI

    torch.set_float32_matmul_precision("high")
    CLI(main)
